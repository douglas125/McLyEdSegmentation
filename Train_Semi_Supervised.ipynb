{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard,ModelCheckpoint,EarlyStopping,LearningRateScheduler,Callback\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model\n",
    "# Next the model definition\n",
    "from tensorflow.keras.layers import ZeroPadding2D,Conv2D,Cropping2D,Lambda,Concatenate,BatchNormalization,Activation,Lambda,Input,Reshape,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For testing the generator\n",
    "#import tensorflow as tf\n",
    "#import tensorflow.contrib.eager as tfe\n",
    "#tfe.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semi_supervised_function import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_on_cpu()\n",
    "def _expand(tensor111, dim1, dim2):\n",
    "    \"\"\"\n",
    "    Expands a tensor to have dimensions dim1 and dim2\n",
    "    \"\"\"\n",
    "    ans = tf.keras.backend.repeat_elements(tensor111, dim1, 1)\n",
    "    ans = tf.keras.backend.repeat_elements(ans, dim2, 2)\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser with many different modfications\n",
    "def parser_train(record):\n",
    "    '''Function to parse a TFRecords example'''\n",
    "    \n",
    "    features = tf.parse_single_example(\n",
    "    record,\n",
    "    features={\n",
    "    'height': tf.FixedLenFeature([], tf.int64),\n",
    "    'width': tf.FixedLenFeature([], tf.int64),\n",
    "    'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "    'mask_raw': tf.FixedLenFeature([], tf.string),\n",
    "    'depth': tf.FixedLenFeature([], tf.float32),\n",
    "    'is_train': tf.FixedLenFeature([], tf.float32),\n",
    "    })\n",
    "\n",
    "    image = tf.decode_raw(features['image_raw'], tf.float64)\n",
    "    annotation = tf.decode_raw(features['mask_raw'], tf.uint8)\n",
    "\n",
    "    #height = tf.cast(features['height'], tf.int32)\n",
    "    #width = tf.cast(features['width'], tf.int32)\n",
    "\n",
    "    annotation=tf.to_double(annotation)\n",
    "    annotation = tf.reshape(annotation, (101, 101, 1))\n",
    "    #,features['is_train']\n",
    "    depth=tf.reshape(features['depth'],shape=(1,1,1))\n",
    "    scaling=tf.reshape(features['is_train'],shape=(1,1))\n",
    "    \n",
    "    #we can add all sorts of stuff to the image\n",
    "    image = tf.reshape(image, (101, 101,1))\n",
    "    \n",
    "    \n",
    "    image_=tf.concat(axis=2,values=[image,annotation])\n",
    "    \n",
    "    #same flips for image and annotaition\n",
    "    image_=tf.image.random_flip_left_right(image_)\n",
    "    \n",
    "    #I still want to train on the full size images\n",
    "    if random.randint(0,100) < 50:\n",
    "        image_=tf.random_crop(image_,size=(50,50,2))\n",
    "        image_=tf.image.resize_images(images=image_,size=(101,101))\n",
    "    \n",
    "    \n",
    "    \n",
    "    image_corrupt=image_[:,:,0]\n",
    "    annotation=image_[:,:,1]\n",
    "    annotation=tf.cast(annotation,dtype=tf.int8)\n",
    "    annotation=tf.reshape(annotation, (101, 101, 1))\n",
    "    image_corrupt=tf.reshape(image_corrupt, (101, 101, 1))\n",
    "    \n",
    "    #We make two corrupted versions of the Image. \n",
    "    image_corrupt_1=tf.image.random_brightness(image_corrupt,max_delta=0.1)\n",
    "    #image_corrupt_1=tf.image.random_contrast(image_corrupt_1,lower=0,upper=1)\n",
    "    image_corrupt_1=tf.image.random_jpeg_quality(image_corrupt_1,min_jpeg_quality=90,max_jpeg_quality=100)\n",
    "    image_corrupt_1 =tf.clip_by_value(image_corrupt_1, 0.0, 1.0)\n",
    "    image_corrupt_1=tf.layers.dropout(image_corrupt_1,rate=0.01,training=True)\n",
    "    #image_corrupt_1=tf.image.grayscale_to_rgb(image_corrupt_1)\n",
    "    \n",
    "    image_corrupt_2=tf.image.random_brightness(image_corrupt,max_delta=0.1)\n",
    "    #image_corrupt_2=tf.image.random_contrast(image_corrupt_2,lower=0,upper=1)\n",
    "    image_corrupt_2=tf.image.random_jpeg_quality(image_corrupt_2,min_jpeg_quality=90,max_jpeg_quality=100)\n",
    "    image_corrupt_2 = tf.clip_by_value(image_corrupt_2, 0.0, 1.0)\n",
    "    image_corrupt_2=tf.layers.dropout(image_corrupt_2,rate=0.01,training=True)\n",
    "    #image_corrupt_2=tf.image.grayscale_to_rgb(image_corrupt_2)\n",
    "    \n",
    "    #mage = tf.reshape(image, (101, 101, 1))\n",
    "    return {'image_corrupt_1': image_corrupt_1,'image_corrupt_2': image_corrupt_2, 'depth': depth,'scaling':scaling},annotation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we create 3 Datasets from the 3 TF Records\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "#One with both the supervised and the semi supervised\n",
    "dataset = tf.data.TFRecordDataset(['salty_train.tfrecords','salty_test.tfrecords'])\n",
    "dataset = dataset.map(parser_train).shuffle(buffer_size=18000*2)\n",
    "dataset = dataset.batch(batch_size).repeat()\n",
    "\n",
    "#One Only Supervised train\n",
    "dataset_sup = tf.data.TFRecordDataset(['salty_train.tfrecords'])\n",
    "dataset_sup = dataset_sup.map(parser_train).shuffle(buffer_size=5000)\n",
    "dataset_sup = dataset_sup.batch(batch_size).repeat()\n",
    "\n",
    "#One with the validation set\n",
    "valid = tf.data.TFRecordDataset(['salty_valid.tfrecords'])\n",
    "valid = valid.map(parser).shuffle(buffer_size=5000)\n",
    "valid = valid.batch(batch_size).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for dic,annotation in tfe.Iterator(dataset):\\n    break'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for dic,annotation in tfe.Iterator(dataset):\n",
    "    break'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"plt.imshow(dic['image_corrupt_1'][case,:,:,0])\\n\\nplt.show()\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''plt.imshow(dic['image_corrupt_1'][case,:,:,0])\n",
    "\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"plt.imshow(dic['image_corrupt_2'][case,:,:,0])\\nplt.show()\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''plt.imshow(dic['image_corrupt_2'][case,:,:,0])\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plt.imshow(annotation[case,:,:,0])\\n\\nplt.show()'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''plt.imshow(annotation[case,:,:,0])\n",
    "\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We laod the validation set and a small amount of the training set to memory for validation purposes\n",
    "X_Val,Y_Val=load_to_memory('salty_valid.tfrecords')\n",
    "X_Val_t,Y_Val_t=load_to_memory('salty_train.tfrecords',size=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = K.variable(0)\n",
    "class NewCallback(Callback):\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha       \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x= epoch/100\n",
    "        x= math.exp(-5*(1-x)**2)\n",
    "        print(x)\n",
    "        K.set_value(self.alpha, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRegularization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomRegularization, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self ,x ,mask=None):\n",
    "        ld=x[0]\n",
    "        rd=x[1]\n",
    "        bce=tf.reduce_mean(tf.keras.losses.mean_squared_error(logits,logits_c),reduction_indices=(1,2))\n",
    "        loss2 = alpha*K.sum(bce)\n",
    "        self.add_loss(loss2,x)\n",
    "        #you can output whatever you need, just update output_shape adequately\n",
    "        #But this is probably useful\n",
    "        return bce\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0][0],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred):\n",
    "\n",
    "    bin_crossentropyloss = tf.keras.losses.binary_crossentropy(y_true,y_pred)\n",
    "    #the scale dsupervised loss scale is a normal Keras Input it is one if this case has a label 0 if not\n",
    "    super_loss=tf.reduce_mean(scaling*((1-dice_coef(y_true, y_pred))*0.99+bin_crossentropyloss*0.01),reduction_indices=(1,2))\n",
    "    \n",
    "    #We add unsupervised losses at multiple levels: 1. the final representation resulting form deeplab\n",
    "    #2.) The deepes representation resulting from the deeplabs\n",
    "    #3.) the final logits. \n",
    "    #unsuper_loss_1=1/3*tf.reduce_mean(0.5*tf.keras.losses.mean_squared_error(low_rep,low_rep_c),reduction_indices=(1,2))\n",
    "    #unsuper_loss_2=1/3*tf.reduce_mean(0.5*tf.keras.losses.mean_squared_error(deep_rep,deep_rep_c),reduction_indices=(1,2))\n",
    "    #unsuper_loss_3=0.5*tf.reduce_mean(tf.keras.losses.mean_squared_error(logits,logits_c),reduction_indices=(1,2))\n",
    "    \n",
    "    #The unsupervised factor has to be determined, in the paper they suggest slowly increasing it during training. \n",
    "    return super_loss #+ scale*(unsuper_loss_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepLabv3Plus_tf import Deeplabv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl1=Deeplabv3(weights='pascal_voc', input_tensor=None, input_shape=(128, 128, 3), classes=21, backbone='xception', OS=16, alpha=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two differently augmented images a input\n",
    "image_corrupt_1 = Input((101, 101, 1),name=\"image_corrupt_1\")\n",
    "image_corrupt_2 = Input((101, 101, 1),name=\"image_corrupt_2\")\n",
    "\n",
    "#The scaling input, which is an array of 1 an 0 if a label exists or not \n",
    "scaling = Input((1,1), name=\"scaling\" )\n",
    "\n",
    "#the depth input\n",
    "depth = Input( (1,1,1),name=\"depth\" )\n",
    "\n",
    "#A cnn layer w use to go to 3 dimensions\n",
    "cnn_1=Conv2D(3, (1, 1), activation='relu')\n",
    "\n",
    "#we pad them \n",
    "s = ZeroPadding2D( padding = ((13, 14), (13, 14)) ) (image_corrupt_1)\n",
    "s_c=ZeroPadding2D( padding = ((13, 14), (13, 14)) ) (image_corrupt_2)\n",
    "\n",
    "\n",
    "#we get them to 3 dimensions\n",
    "img_input =  cnn_1(s) \n",
    "img_input_c=cnn_1(s_c) \n",
    "\n",
    "#Do forward pass with different dropout configurations Im pretty sure this should work out as I had hoped, ( cause of the named stuff)\n",
    "extract=dl1(img_input)\n",
    "extract_c=dl1(img_input_c)\n",
    "extract=Dropout(0.01)(extract)\n",
    "extract_c=Dropout(0.01)(extract_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#We add depth\n",
    "dd = Lambda(lambda x: x * 0.001) (depth)\n",
    "dd = Lambda(lambda x: _expand(x, 128, 128) )(dd)\n",
    "\n",
    "#We define a \"get logits\" function. \n",
    "\n",
    "\n",
    "cn_l=Concatenate(axis=3,name = \"con_last\")\n",
    "cnv_l=Conv2D(128,(1,1),activation=\"relu\",name = \"conv_last\")\n",
    "cnv_ll=Conv2D(1,(1,1),name = \"logits_last\",activation=\"sigmoid\")\n",
    "crop_ll=Cropping2D(cropping=((13, 14), (13, 14)),name = \"crop_last\" )\n",
    "def get_logits(extract):\n",
    "    x_2=cn_l([extract,dd])\n",
    "    x_2=cnv_l(x_2)\n",
    "    logits=cnv_ll(x_2)\n",
    "    logits =  crop_ll(logits)\n",
    "    return logits\n",
    "\n",
    "#We ge tthe logits from both\n",
    "logits=get_logits(extract)\n",
    "logits_c=get_logits(extract_c)\n",
    "cr = CustomRegularization()([logits,logits_c])\n",
    "\n",
    "model=Model(inputs=[image_corrupt_1,image_corrupt_2,depth,scaling],outputs=[logits,cr])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image_corrupt_1 (InputLayer)    (None, 101, 101, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "image_corrupt_2 (InputLayer)    (None, 101, 101, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPadding2D (None, 128, 128, 1)  0           image_corrupt_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPadding2D (None, 128, 128, 1)  0           image_corrupt_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 3)  6           zero_padding2d_6[0][0]           \n",
      "                                                                 zero_padding2d_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "depth (InputLayer)              (None, 1, 1, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "deeplabv3p (Model)              (None, 128, 128, 21) 41258213    conv2d[0][0]                     \n",
      "                                                                 conv2d[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 1, 1, 1)      0           depth[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128, 128, 21) 0           deeplabv3p[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 128, 128, 1)  0           lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128, 128, 21) 0           deeplabv3p[2][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "con_last (Concatenate)          (None, 128, 128, 22) 0           dropout[0][0]                    \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_last (Conv2D)              (None, 128, 128, 128 2944        con_last[0][0]                   \n",
      "                                                                 con_last[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "logits_last (Conv2D)            (None, 128, 128, 1)  129         conv_last[0][0]                  \n",
      "                                                                 conv_last[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "crop_last (Cropping2D)          (None, 101, 101, 1)  0           logits_last[0][0]                \n",
      "                                                                 logits_last[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "custom_regularization (CustomRe (None,)              0           crop_last[0][0]                  \n",
      "                                                                 crop_last[1][0]                  \n",
      "==================================================================================================\n",
      "Total params: 41,261,292\n",
      "Trainable params: 41,058,492\n",
      "Non-trainable params: 202,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntervalEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(),training_data=() ,interval=10):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        #also load training data\n",
    "        self.X_Val_T,self.Y_Val_T =training_data\n",
    "        self.score_list=[0]\n",
    "        self.score_list_train=[0]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=1)[0]\n",
    "            y_pred_t = self.model.predict(self.X_Val_T, verbose=1)[0]\n",
    "            #first we have to threshhold\n",
    "            prec_score=return_iou(y_pred,self.y_val)\n",
    "            prec_score_t=return_iou(y_pred_t,self.Y_Val_T)\n",
    "            \n",
    "            print('Validation score is {}.Train score is {} Best so far is {}'.format(prec_score,prec_score_t, max(self.score_list)))\n",
    "            \n",
    "            if prec_score > max(self.score_list):\n",
    "                print('Saving model-tgs-salt-IV.h5')\n",
    "                self.model.save_weights('model-tgs-salt-IV.h5')\n",
    "            \n",
    "            self.score_list.append(prec_score)\n",
    "            self.score_list_train.append(prec_score_t)\n",
    "            \n",
    "            \n",
    "def _expand(tensor111, dim1, dim2):\n",
    "    \"\"\"\n",
    "    Expands a tensor to have dimensions dim1 and dim2\n",
    "    \"\"\"\n",
    "    ans = tf.keras.backend.repeat_elements(tensor111, dim1, 1)\n",
    "    ans = tf.keras.backend.repeat_elements(ans, dim2, 2)\n",
    "    return ans\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.001 \n",
    "    drop = 0.95\n",
    "    epochs_drop = 5.0\n",
    "    lrate = initial_lrate * math.pow(drop,  \n",
    "            math.floor((1+epoch)/epochs_drop))\n",
    "    \n",
    "    if (lrate < 5e-7):\n",
    "        lrate = 5e-7\n",
    "      \n",
    "    print('Changing learning rate to {}'.format(lrate))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Callbacks\n",
    "#1- Tensorboard\n",
    "tens=TensorBoard(log_dir='../logs/semi_upsampled_mobilenet_ramping_onlly_preds_cropping_correct_loss_high_lr', histogram_freq=0, batch_size=32)\n",
    "#LR Sched\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "#THe evaluation too. \n",
    "internalEval = IntervalEvaluation( validation_data=(X_Val,Y_Val),training_data= (X_Val_t,Y_Val_t),interval = 1 )\n",
    "#Model Evaluation \n",
    "check=ModelCheckpoint(filepath=\"semi_upsampled_mobilenet_ramping_onlly_preds_cropping_correct_loss_high_lr.h5\",save_best_only=True,monitor=\"val_loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000\n",
      "Changing learning rate to 0.001\n",
      "500/500 [==============================] - 1098s 2s/step - loss: 0.0328 - crop_last_loss: 0.0328 - val_loss: 0.0398 - val_crop_last_loss: 0.0398\n",
      "160/160 [==============================] - 19s 117ms/step\n",
      "160/160 [==============================] - 5s 32ms/step\n",
      "Validation score is 0.60625.Train score is 0.62625 Best so far is 0\n",
      "Saving model-tgs-salt-IV.h5\n",
      "0.007442860710056644\n",
      "Epoch 3/1000\n",
      "Changing learning rate to 0.001\n",
      " 33/500 [>.............................] - ETA: 14:31 - loss: 0.0237 - crop_last_loss: 0.0229"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss=[dice_loss,None])\n",
    "#fit for one epoch \n",
    "model.fit(dataset,steps_per_epoch=int((8000)/batch_size), epochs=1000,validation_data=valid,validation_steps=20,callbacks=[tens,lrate,check,internalEval,NewCallback(alpha)],initial_epoch=1)#18000+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range (3,500):\n",
    "#Get the modifier for the loss \n",
    "    x= j/100\n",
    "    x=math.exp(-5*(1-x)**2)\n",
    "    print(x)\n",
    "    scale=tf.convert_to_tensor(x,dtype=tf.float32,name=\"scale_Loss\")\n",
    "    #recompile loss with new scaling term \n",
    "    model.compile(optimizer='adam', loss=[dice_loss,zero_loss])\n",
    "    #fit for one epoch \n",
    "    model.fit(dataset,steps_per_epoch=int((8000)/batch_size), epochs=j+1,validation_data=valid,validation_steps=20,callbacks=[tens,lrate,check,internalEval],initial_epoch=j)#18000+\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In a way at each epoch we \"sample\" 4k true example and 4k unlabeled examples. \n",
    "\n",
    "model.compile(optimizer='adam', loss=[dice_loss])\n",
    "model.fit(dataset,steps_per_epoch=int((8000)/batch_size), epochs=1000,validation_data=valid,validation_steps=20,callbacks=[tens,lrate,check,internalEval])#18000+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
