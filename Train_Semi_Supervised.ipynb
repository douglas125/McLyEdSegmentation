{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard,ModelCheckpoint,EarlyStopping,LearningRateScheduler,Callback\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model\n",
    "from DeepLabv3Plus_tf import deeplab_forward\n",
    "# Next the model definition\n",
    "from tensorflow.keras.layers import ZeroPadding2D,Conv2D,Cropping2D,Lambda,Concatenate,BatchNormalization,Activation,Lambda,Input,Reshape,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semi_supervised_function import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_on_cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we create 3 Datasets from the 3 TF Records\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "#One with both the supervised and the semi supervised\n",
    "dataset = tf.data.TFRecordDataset(['salty_train.tfrecords','salty_test.tfrecords'])\n",
    "dataset = dataset.map(parser_train).shuffle(buffer_size=18000*2)\n",
    "dataset = dataset.batch(batch_size).repeat()\n",
    "\n",
    "#One Only Supervised train\n",
    "dataset_sup = tf.data.TFRecordDataset(['salty_train.tfrecords'])\n",
    "dataset_sup = dataset_sup.map(parser_train).shuffle(buffer_size=5000)\n",
    "dataset_sup = dataset_sup.batch(batch_size).repeat()\n",
    "\n",
    "#One with the validation set\n",
    "valid = tf.data.TFRecordDataset(['salty_valid.tfrecords'])\n",
    "valid = valid.map(parser).shuffle(buffer_size=5000)\n",
    "valid = valid.batch(batch_size).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We laod the validation set and a small amount of the training set to memory for validation purposes\n",
    "X_Val,Y_Val=load_to_memory('salty_valid.tfrecords')\n",
    "X_Val_t,Y_Val_t=load_to_memory('salty_train.tfrecords',size=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two differently augmented images a input\n",
    "image_corrupt_1 = Input((101, 101, 1),name=\"image_corrupt_1\")\n",
    "image_corrupt_2 = Input((101, 101, 1),name=\"image_corrupt_2\")\n",
    "\n",
    "#The scaling input, which is an array of 1 an 0 if a label exists or not \n",
    "scaling = Input((1,1), name=\"scaling\" )\n",
    "\n",
    "#the depth input\n",
    "depth = Input( (1,1,1),name=\"depth\" )\n",
    "\n",
    "#A cnn layer w use to go to 3 dimensions\n",
    "cnn_1=Conv2D(3, (1, 1), activation='relu')\n",
    "\n",
    "#we pad them \n",
    "s = ZeroPadding2D( padding = ((13, 14), (13, 14)) ) (image_corrupt_1)\n",
    "s_c=ZeroPadding2D( padding = ((13, 14), (13, 14)) ) (image_corrupt_2)\n",
    "\n",
    "#we get them to 3 dimensions\n",
    "img_input =  cnn_1(s) \n",
    "img_input_c=cnn_1(s_c) \n",
    "\n",
    "#Do forward pass with different dropout configurations\n",
    "extract,low_rep,deep_rep=deeplab_forward(img_input,backbone=\"mobilenetv2\",drop=1)\n",
    "extract_c,low_rep_c,deep_rep_c=deeplab_forward(img_input_c,backbone=\"mobilenetv2\",drop=0)\n",
    "\n",
    "#We add depth\n",
    "dd = Lambda(lambda x: x * 0.001) (depth)\n",
    "dd = Lambda(lambda x: _expand(x, 128, 128) )(dd)\n",
    "\n",
    "#We define a \"get logits\" function. \n",
    "\n",
    "def get_logits(extract):\n",
    "    x_2=Concatenate(axis=3,name = \"con_last\")([extract,dd])\n",
    "    x_2=Conv2D(64,(1,1),activation=\"relu\",name = \"conv_last\")(x_2)\n",
    "    logits=Conv2D(1,(1,1),name = \"logits_last\")(x_2)\n",
    "    logits = Cropping2D(cropping=((13, 14), (13, 14)),name = \"crop_last\" ) (logits)\n",
    "    return logits\n",
    "\n",
    "#We ge tthe logits from both\n",
    "logits=get_logits(extract)\n",
    "logits_c=get_logits(extract_c)\n",
    "\n",
    "#we get the sigmoids\n",
    "sigmoids=Activation(activation=\"sigmoid\")(logits)\n",
    "model=Model(inputs=[image_corrupt_1,image_corrupt_2,depth,scaling],outputs=[sigmoids])\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import get_file\n",
    "def load_weights(backbone=\"xception\"):\n",
    "    if backbone == \"mobilenetv2\":\n",
    "        WEIGHTS_PATH_MOBILE = \"https://github.com/bonlime/keras-deeplab-v3-plus/releases/download/1.1/deeplabv3_mobilenetv2_tf_dim_ordering_tf_kernels.h5\"\n",
    "        weights_path = get_file('deeplabv3_mobilenetv2_tf_dim_ordering_tf_kernels.h5',\n",
    "                                WEIGHTS_PATH_MOBILE,\n",
    "                                cache_subdir='models')\n",
    "        \n",
    "    if backbone == \"xception\":\n",
    "        WEIGHTS_PATH_X = \"https://github.com/bonlime/keras-deeplab-v3-plus/releases/download/1.1/deeplabv3_xception_tf_dim_ordering_tf_kernels.h5\"\n",
    "\n",
    "        weights_path = get_file('deeplabv3_xception_tf_dim_ordering_tf_kernels.h5',\n",
    "                                WEIGHTS_PATH_X,\n",
    "                                cache_subdir='models')\n",
    "\n",
    "    model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "load_weights(backbone = \"mobilenetv2\")\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred):\n",
    "\n",
    "    bin_crossentropyloss = tf.keras.losses.binary_crossentropy(y_true,y_pred)\n",
    "    #the scale dsupervised loss scale is a normal Keras Input it is one if this case has a label 0 if not\n",
    "    super_loss=tf.reduce_mean(scaling*((1-dice_coef(y_true, y_pred))*0.99+bin_crossentropyloss*0.01),reduction_indices=(1,2))\n",
    "    \n",
    "    #We add unsupervised losses at multiple levels: 1. the final representation resulting form deeplab\n",
    "    #2.) The deepes representation resulting from the deeplabs\n",
    "    #3.) the final logits. \n",
    "    unsuper_loss_1=1/3*tf.reduce_mean(0.5*tf.keras.losses.mean_squared_error(low_rep,low_rep_c),reduction_indices=(1,2))\n",
    "    unsuper_loss_2=1/3*tf.reduce_mean(0.5*tf.keras.losses.mean_squared_error(deep_rep,deep_rep_c),reduction_indices=(1,2))\n",
    "    unsuper_loss_3=1/3*tf.reduce_mean(0.5*tf.keras.losses.mean_squared_error(logits,logits_c),reduction_indices=(1,2))\n",
    "    \n",
    "    #The unsupervised factor has to be determined, in the paper they suggest slowly increasing it during training. \n",
    "    return super_loss + scale*(unsuper_loss_1+unsuper_loss_2+unsuper_loss_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Callbacks\n",
    "#1- Tensorboard\n",
    "tens=TensorBoard(log_dir='../logs/semi_upsampled_mobilenet_ramping', histogram_freq=0, batch_size=32)\n",
    "#LR Sched\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "#THe evaluation too. \n",
    "internalEval = IntervalEvaluation( validation_data=(X_Val,Y_Val),training_data= (X_Val_t,Y_Val_t),interval = 1 )\n",
    "#Model Evaluation \n",
    "check=ModelCheckpoint(filepath=\"semi_upsampled_xcept_0.001.h5\",save_best_only=True,monitor=\"val_loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      "Changing learning rate to 0.001\n",
      "250/250 [==============================] - 444s 2s/step - loss: 0.0365 - val_loss: 0.3097\n",
      "160/160 [==============================] - 8s 53ms/step\n",
      "160/160 [==============================] - 1s 6ms/step\n",
      "Validation score is 0.15499999999999997.Train score is 0.20187500000000003 Best so far is 0.403125\n",
      "Epoch 3/3\n",
      "Changing learning rate to 0.001\n",
      "250/250 [==============================] - 448s 2s/step - loss: 0.0334 - val_loss: 0.1579\n",
      "160/160 [==============================] - 10s 61ms/step\n",
      "160/160 [==============================] - 1s 6ms/step\n",
      "Validation score is 0.20437500000000003.Train score is 0.231875 Best so far is 0.403125\n",
      "Epoch 4/4\n",
      "Changing learning rate to 0.001\n",
      " 96/250 [==========>...................] - ETA: 5:55 - loss: 0.0340"
     ]
    }
   ],
   "source": [
    "for j in range (1,500):\n",
    "#Get the modifier for the loss \n",
    "    x= j/100\n",
    "    x=math.exp(-5*(1-x)**2)\n",
    "    scale=tf.convert_to_tensor(x,dtype=tf.float32,name=\"scale_Loss\")\n",
    "    #recompile loss with new scaling term \n",
    "    model.compile(optimizer='adam', loss=[dice_loss])\n",
    "    #fit for one epoch \n",
    "    model.fit(dataset,steps_per_epoch=int((8000)/batch_size), epochs=j+1,validation_data=valid,validation_steps=20,callbacks=[tens,lrate,check,internalEval],initial_epoch=j)#18000+\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In a way at each epoch we \"sample\" 4k true example and 4k unlabeled examples. \n",
    "\n",
    "model.compile(optimizer='adam', loss=[dice_loss])\n",
    "model.fit(dataset,steps_per_epoch=int((8000)/batch_size), epochs=1000,validation_data=valid,validation_steps=20,callbacks=[tens,lrate,check,internalEval])#18000+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
